{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e74c4dd4",
      "metadata": {
        "id": "e74c4dd4"
      },
      "source": [
        "\n",
        "# ğŸ¨ Neural Style **Transfer**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d8d31a17",
      "metadata": {
        "id": "d8d31a17"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14084615",
      "metadata": {
        "id": "14084615"
      },
      "source": [
        "# Upload images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a30c6044",
      "metadata": {
        "id": "a30c6044"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path, max_size=256):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    if max_size:\n",
        "        if max(image.size) > max_size:\n",
        "            size = max_size\n",
        "            image = image.resize((size, int(size * image.size[1] / image.size[0])), Image.LANCZOS)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = transform(image).unsqueeze(0)  #batch\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0f206c",
      "metadata": {
        "id": "3e0f206c"
      },
      "source": [
        "2.1 Load the pre-trained VGG19 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "3b7bc421",
      "metadata": {
        "id": "3b7bc421"
      },
      "outputs": [],
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    \"\"\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø·Ø¨Ù‚Ø§Øª Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\"\"\"\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1',\n",
        "                  '10': 'conv3_1',\n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',\n",
        "                  '28': 'conv5_1'}\n",
        "\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74e7423",
      "metadata": {
        "id": "c74e7423"
      },
      "source": [
        "2.2 ØªØ¹Ø±ÙŠÙ Ø¯ÙˆØ§Ù„ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© (Content Loss & Style Loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f820b204",
      "metadata": {
        "id": "f820b204"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    \"\"\"Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© ØºØ±Ø§Ù… Ù„Ù„ØªÙ†Ø³ÙˆØ±\"\"\"\n",
        "    batch_size, channels, h, w = tensor.size()\n",
        "    tensor = tensor.view(batch_size * channels, h * w)\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    return gram.div(channels * h * w)\n",
        "\n",
        "def content_loss(target_features, content_features):\n",
        "    \"\"\"Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰\"\"\"\n",
        "    return F.mse_loss(target_features, content_features)\n",
        "\n",
        "def style_loss(target_features, style_features):\n",
        "    \"\"\"Ø­Ø³Ø§Ø¨ Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ø£Ø³Ù„ÙˆØ¨\"\"\"\n",
        "    target_gram = gram_matrix(target_features)\n",
        "    style_gram = gram_matrix(style_features)\n",
        "    return F.mse_loss(target_gram, style_gram)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "myxEP6WvnaQj",
      "metadata": {
        "id": "myxEP6WvnaQj"
      },
      "outputs": [],
      "source": [
        "def style_transfer(content_image, style_image, num_steps=300,\n",
        "                   style_weight=1e4, content_weight=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    content_image = content_image.to(device)\n",
        "    style_image = style_image.to(device)\n",
        "\n",
        "    # model VGG\n",
        "    vgg = models.vgg19(pretrained=True).features\n",
        "    for param in vgg.parameters():\n",
        "        param.requires_grad_(False)\n",
        "    vgg.to(device)\n",
        "\n",
        "    content_layers = ['conv4_2']\n",
        "    style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
        "\n",
        "    # gradient\n",
        "    target_image = content_image.clone().requires_grad_(True).to(device)\n",
        "\n",
        "    optimizer = optim.Adam([target_image], lr=0.01)\n",
        "\n",
        "    content_features = get_features(content_image, vgg)\n",
        "    style_features = get_features(style_image, vgg)\n",
        "\n",
        "    for step in range(1, num_steps + 1):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target_features = get_features(target_image, vgg)\n",
        "\n",
        "        c_loss = 0\n",
        "        for layer in content_layers:\n",
        "            c_loss += content_loss(target_features[layer], content_features[layer])\n",
        "\n",
        "        s_loss = 0\n",
        "        for layer in style_layers:\n",
        "            s_loss += style_loss(target_features[layer], style_features[layer])\n",
        "\n",
        "        total_loss = content_weight * c_loss + style_weight * s_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}: Content Loss = {c_loss.item():.4f}, Style Loss = {s_loss.item():.4f}\")\n",
        "\n",
        "    target_image.data.clamp_(0, 1)\n",
        "    return target_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13fef2b",
      "metadata": {
        "id": "b13fef2b"
      },
      "source": [
        "2.4 Try a fast expermant (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "8f69c4a6",
      "metadata": {
        "id": "8f69c4a6"
      },
      "outputs": [],
      "source": [
        "class TransformerNet(nn.Module):\n",
        "    \"\"\"Ø´Ø¨ÙƒØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹\"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "\n",
        "        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ±Ù…ÙŠØ² (Encoder)\n",
        "        self.enc1 = nn.Conv2d(3, 32, kernel_size=9, stride=1, padding=4)\n",
        "        self.enc2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.enc3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Ø§Ù„ÙƒØªÙ„ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© (Residual Blocks)\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "\n",
        "        # Ø·Ø¨Ù‚Ø§Øª ÙÙƒ Ø§Ù„ØªØ±Ù…ÙŠØ² (Decoder)\n",
        "        self.dec1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.dec2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.dec3 = nn.Conv2d(32, 3, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ØªÙ…Ø±ÙŠØ± Ø£Ù…Ø§Ù…ÙŠ\n",
        "        x = F.relu(self.enc1(x))\n",
        "        x = F.relu(self.enc2(x))\n",
        "        x = F.relu(self.enc3(x))\n",
        "\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.res5(x)\n",
        "\n",
        "        x = F.relu(self.dec1(x))\n",
        "        x = F.relu(self.dec2(x))\n",
        "        x = self.dec3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"ÙƒØªÙ„Ø© Ù…ØªØ¨Ù‚ÙŠØ© Ù„Ù„Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.in1(self.conv1(x)))\n",
        "        x = self.in2(self.conv2(x))\n",
        "        return x + residual\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e410660",
      "metadata": {
        "id": "1e410660"
      },
      "source": [
        "3.1 optional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "6976f5a5",
      "metadata": {
        "id": "6976f5a5"
      },
      "outputs": [],
      "source": [
        "def experiment_with_weights(content_image, style_image):\n",
        "    \"\"\"ØªØ¬Ø±Ø¨Ø© Ù‚ÙŠÙ… Ù…Ø®ØªÙ„ÙØ© Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ø£Ø³Ù„ÙˆØ¨\"\"\"\n",
        "    content_weights = [1, 10, 100]\n",
        "    style_weights = [1e4, 1e5, 1e6, 1e7]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for c_weight in content_weights:\n",
        "        for s_weight in style_weights:\n",
        "            print(f\"Testing with content_weight={c_weight}, style_weight={s_weight}\")\n",
        "            result = style_transfer(\n",
        "                content_image,\n",
        "                style_image,\n",
        "                num_steps=200,\n",
        "                content_weight=c_weight,\n",
        "                style_weight=s_weight\n",
        "            )\n",
        "            results[(c_weight, s_weight)] = result\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c54ae6",
      "metadata": {
        "id": "63c54ae6"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b967fdfa",
      "metadata": {
        "id": "b967fdfa"
      },
      "source": [
        "4.1 to visulaise the orignal imgaes(opintail)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "56bc9d62",
      "metadata": {
        "id": "56bc9d62"
      },
      "outputs": [],
      "source": [
        "def display_results(original, style, results):\n",
        "    \"\"\"Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ø­ÙˆÙ„Ø© Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨\"\"\"\n",
        "    fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
        "\n",
        "    ax[0, 0].imshow(to_pil_image(original[0]))\n",
        "    ax[0, 0].set_title(\"Original Content\")\n",
        "    ax[0, 1].imshow(to_pil_image(style[0]))\n",
        "    ax[0, 1].set_title(\"Style Reference\")\n",
        "    ax[0, 2].axis('off')\n",
        "\n",
        "    row, col = 1, 0\n",
        "    for name, img in results.items():\n",
        "        ax[row, col].imshow(to_pil_image(img[0]))\n",
        "        ax[row, col].set_title(f\"Style Weight: {name}\")\n",
        "\n",
        "        col += 1\n",
        "        if col > 2:\n",
        "            col = 0\n",
        "            row += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_images_from_folder(folder_path, max_size=None):\n",
        "    images = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            img = load_image(path, max_size)\n",
        "            images[filename] = img\n",
        "    return images"
      ],
      "metadata": {
        "id": "r5Wrhb3eHxFY"
      },
      "id": "r5Wrhb3eHxFY",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "72ca2a58",
      "metadata": {
        "id": "72ca2a58"
      },
      "outputs": [],
      "source": [
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.clone().detach()\n",
        "    image = image.squeeze(0)\n",
        "    image = image.numpy().transpose(1, 2, 0)\n",
        "    image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "    image = np.clip(image, 0, 1)\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "c6eada70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "c6eada70",
        "outputId": "64edc6b0-e777-4f5c-a910-aecab2895886"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª\\ncontent_images = load_images_from_folder(\"/content/con img\", max_size=512)\\nstyle_images = load_images_from_folder(\"/content/style img\", max_size=512)\\n\\n# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµÙˆØ±\\ncontent_img = content_images[\"natural.png\"]\\nstyle_img = style_images[\"dog.jpg\"]\\n\\n# ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£ÙˆØ²Ø§Ù†\\noutput_image = style_transfer(\\n    content_img, style_img,\\n    num_steps=500,\\n    style_weight=1e5,  # ØªÙˆØ§Ø²Ù† Ø£ÙØ¶Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰\\n    content_weight=1\\n)\\nimshow(output_image, title=\"Stylized Result - Balanced Weights\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "'''\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª\n",
        "content_images = load_images_from_folder(\"/content/con img\", max_size=512)\n",
        "style_images = load_images_from_folder(\"/content/style img\", max_size=512)\n",
        "\n",
        "# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµÙˆØ±\n",
        "content_img = content_images[\"natural.png\"]\n",
        "style_img = style_images[\"dog.jpg\"]\n",
        "\n",
        "# ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£ÙˆØ²Ø§Ù†\n",
        "output_image = style_transfer(\n",
        "    content_img, style_img,\n",
        "    num_steps=500,\n",
        "    style_weight=1e5,  # ØªÙˆØ§Ø²Ù† Ø£ÙØ¶Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰\n",
        "    content_weight=1\n",
        ")\n",
        "imshow(output_image, title=\"Stylized Result - Balanced Weights\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "xTj5xvdsqE9v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "xTj5xvdsqE9v",
        "outputId": "c848e5ee-e161-4fcf-885e-c313cf0c3268"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª\\ncontent_images = load_images_from_folder(\"/content/con img\", max_size=512)\\nstyle_images = load_images_from_folder(\"/content/style img\", max_size=512)\\n\\n# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµÙˆØ±\\ncontent_img = content_images[\"portrate.png\"]\\nstyle_img = style_images[\"Portrait â€“ Picasso.png\"]\\n\\n# ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£ÙˆØ²Ø§Ù†\\noutput_image = style_transfer(\\n    content_img, style_img,\\n    num_steps=500,\\n    style_weight=1e5,  # ØªÙˆØ§Ø²Ù† Ø£ÙØ¶Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰\\n    content_weight=1\\n)\\nimshow(output_image, title=\"Stylized Result - Balanced Weights\")\"\"\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "'''\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª\n",
        "content_images = load_images_from_folder(\"/content/con img\", max_size=512)\n",
        "style_images = load_images_from_folder(\"/content/style img\", max_size=512)\n",
        "\n",
        "# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµÙˆØ±\n",
        "content_img = content_images[\"portrate.png\"]\n",
        "style_img = style_images[\"Portrait â€“ Picasso.png\"]\n",
        "\n",
        "# ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ÙŠ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£ÙˆØ²Ø§Ù†\n",
        "output_image = style_transfer(\n",
        "    content_img, style_img,\n",
        "    num_steps=500,\n",
        "    style_weight=1e5,  # ØªÙˆØ§Ø²Ù† Ø£ÙØ¶Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰\n",
        "    content_weight=1\n",
        ")\n",
        "imshow(output_image, title=\"Stylized Result - Balanced Weights\")\"\"\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iC96zqU6vT0p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC96zqU6vT0p",
        "outputId": "604575b1-ffe3-4d1a-9727-8d67dc282518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: beach.png + Water Lilies â€“ Monet.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:02<00:00, 195MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50: Content Loss = 0.4043, Style Loss = 0.0000\n",
            "Step 100: Content Loss = 0.3512, Style Loss = 0.0000\n",
            "Step 150: Content Loss = 0.3266, Style Loss = 0.0000\n",
            "Step 200: Content Loss = 0.3116, Style Loss = 0.0000\n",
            "Step 250: Content Loss = 0.3004, Style Loss = 0.0000\n",
            "Step 300: Content Loss = 0.2930, Style Loss = 0.0000\n",
            "Saved to /content/stylized_outputs/beach_with_Water Lilies â€“ Monet.jpg\n",
            "\n",
            "Processing: city.jpg + Starry Night â€“ Van Gogh.jpg\n",
            "Step 50: Content Loss = 5.1864, Style Loss = 0.0001\n",
            "Step 100: Content Loss = 4.3684, Style Loss = 0.0000\n"
          ]
        }
      ],
      "source": [
        "selected_pairs = [\n",
        "    (\"beach.png\", \"Water Lilies â€“ Monet.jpg\"),\n",
        "    (\"city.jpg\", \"Starry Night â€“ Van Gogh.jpg\"),\n",
        "    (\"natural.png\", \"Mosaic Style.jpg\"),\n",
        "    (\"portrate.png\", \"Portrait â€“ Picasso.jpg\"),\n",
        "    (\"street.png\", \"The Scream â€“ Munch.jpg\"),\n",
        "]\n",
        "\n",
        "import os\n",
        "output_dir = \"/content/stylized_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "content_images = load_images_from_folder(\"/content/con img\", max_size=512)\n",
        "style_images = load_images_from_folder(\"/content/style img\", max_size=512)\n",
        "\n",
        "\n",
        "for content_name, style_name in selected_pairs:\n",
        "    print(f\"Processing: {content_name} + {style_name}\")\n",
        "    # Access content_images and style_images\n",
        "    content_img = content_images[content_name]\n",
        "    style_img = style_images[style_name]\n",
        "    output = style_transfer(\n",
        "        content_img, style_img,\n",
        "        num_steps=300, style_weight=1e5, content_weight=1\n",
        "    )\n",
        "\n",
        "    result = output.clone().detach().squeeze(0)\n",
        "    result = result.cpu().numpy().transpose(1, 2, 0)\n",
        "    result = result * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "    result = np.clip(result, 0, 1)\n",
        "    from PIL import Image\n",
        "    result_img = Image.fromarray((result * 255).astype('uint8'))\n",
        "    save_path = os.path.join(output_dir, f\"{content_name.split('.')[0]}_with_{style_name.split('.')[0]}.jpg\")\n",
        "    result_img.save(save_path)\n",
        "    print(f\"Saved to {save_path}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}