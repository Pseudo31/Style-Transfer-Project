{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e74c4dd4",
      "metadata": {
        "id": "e74c4dd4"
      },
      "source": [
        "\n",
        "# 🎨 Neural Style **Transfer**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d8d31a17",
      "metadata": {
        "id": "d8d31a17"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14084615",
      "metadata": {
        "id": "14084615"
      },
      "source": [
        "# Upload images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a30c6044",
      "metadata": {
        "id": "a30c6044"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path, max_size=256):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    if max_size:\n",
        "        if max(image.size) > max_size:\n",
        "            size = max_size\n",
        "            image = image.resize((size, int(size * image.size[1] / image.size[0])), Image.LANCZOS)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = transform(image).unsqueeze(0)  #batch\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0f206c",
      "metadata": {
        "id": "3e0f206c"
      },
      "source": [
        "2.1 Load the pre-trained VGG19 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "3b7bc421",
      "metadata": {
        "id": "3b7bc421"
      },
      "outputs": [],
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    \"\"\"استخراج خصائص الصورة من طبقات محددة في النموذج\"\"\"\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1',\n",
        "                  '10': 'conv3_1',\n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',\n",
        "                  '28': 'conv5_1'}\n",
        "\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74e7423",
      "metadata": {
        "id": "c74e7423"
      },
      "source": [
        "2.2 تعريف دوال حساب الخسارة (Content Loss & Style Loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f820b204",
      "metadata": {
        "id": "f820b204"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    \"\"\"حساب مصفوفة غرام للتنسور\"\"\"\n",
        "    batch_size, channels, h, w = tensor.size()\n",
        "    tensor = tensor.view(batch_size * channels, h * w)\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    return gram.div(channels * h * w)\n",
        "\n",
        "def content_loss(target_features, content_features):\n",
        "    \"\"\"حساب خسارة المحتوى\"\"\"\n",
        "    return F.mse_loss(target_features, content_features)\n",
        "\n",
        "def style_loss(target_features, style_features):\n",
        "    \"\"\"حساب خسارة الأسلوب\"\"\"\n",
        "    target_gram = gram_matrix(target_features)\n",
        "    style_gram = gram_matrix(style_features)\n",
        "    return F.mse_loss(target_gram, style_gram)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "myxEP6WvnaQj",
      "metadata": {
        "id": "myxEP6WvnaQj"
      },
      "outputs": [],
      "source": [
        "def style_transfer(content_image, style_image, num_steps=300,\n",
        "                   style_weight=1e4, content_weight=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    content_image = content_image.to(device)\n",
        "    style_image = style_image.to(device)\n",
        "\n",
        "    # model VGG\n",
        "    vgg = models.vgg19(pretrained=True).features\n",
        "    for param in vgg.parameters():\n",
        "        param.requires_grad_(False)\n",
        "    vgg.to(device)\n",
        "\n",
        "    content_layers = ['conv4_2']\n",
        "    style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
        "\n",
        "    # gradient\n",
        "    target_image = content_image.clone().requires_grad_(True).to(device)\n",
        "\n",
        "    optimizer = optim.Adam([target_image], lr=0.01)\n",
        "\n",
        "    content_features = get_features(content_image, vgg)\n",
        "    style_features = get_features(style_image, vgg)\n",
        "\n",
        "    for step in range(1, num_steps + 1):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target_features = get_features(target_image, vgg)\n",
        "\n",
        "        c_loss = 0\n",
        "        for layer in content_layers:\n",
        "            c_loss += content_loss(target_features[layer], content_features[layer])\n",
        "\n",
        "        s_loss = 0\n",
        "        for layer in style_layers:\n",
        "            s_loss += style_loss(target_features[layer], style_features[layer])\n",
        "\n",
        "        total_loss = content_weight * c_loss + style_weight * s_loss\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}: Content Loss = {c_loss.item():.4f}, Style Loss = {s_loss.item():.4f}\")\n",
        "\n",
        "    target_image.data.clamp_(0, 1)\n",
        "    return target_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13fef2b",
      "metadata": {
        "id": "b13fef2b"
      },
      "source": [
        "2.4 Try a fast expermant (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "8f69c4a6",
      "metadata": {
        "id": "8f69c4a6"
      },
      "outputs": [],
      "source": [
        "class TransformerNet(nn.Module):\n",
        "    \"\"\"شبكة التحويل للنقل الأسلوبي السريع\"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "\n",
        "        # طبقات الترميز (Encoder)\n",
        "        self.enc1 = nn.Conv2d(3, 32, kernel_size=9, stride=1, padding=4)\n",
        "        self.enc2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.enc3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # الكتل المتبقية (Residual Blocks)\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "\n",
        "        # طبقات فك الترميز (Decoder)\n",
        "        self.dec1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.dec2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.dec3 = nn.Conv2d(32, 3, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # تمرير أمامي\n",
        "        x = F.relu(self.enc1(x))\n",
        "        x = F.relu(self.enc2(x))\n",
        "        x = F.relu(self.enc3(x))\n",
        "\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.res5(x)\n",
        "\n",
        "        x = F.relu(self.dec1(x))\n",
        "        x = F.relu(self.dec2(x))\n",
        "        x = self.dec3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"كتلة متبقية للنقل الأسلوبي السريع\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.in1(self.conv1(x)))\n",
        "        x = self.in2(self.conv2(x))\n",
        "        return x + residual\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e410660",
      "metadata": {
        "id": "1e410660"
      },
      "source": [
        "3.1 optional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "6976f5a5",
      "metadata": {
        "id": "6976f5a5"
      },
      "outputs": [],
      "source": [
        "def experiment_with_weights(content_image, style_image):\n",
        "    \"\"\"تجربة قيم مختلفة لأوزان المحتوى والأسلوب\"\"\"\n",
        "    content_weights = [1, 10, 100]\n",
        "    style_weights = [1e4, 1e5, 1e6, 1e7]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for c_weight in content_weights:\n",
        "        for s_weight in style_weights:\n",
        "            print(f\"Testing with content_weight={c_weight}, style_weight={s_weight}\")\n",
        "            result = style_transfer(\n",
        "                content_image,\n",
        "                style_image,\n",
        "                num_steps=200,\n",
        "                content_weight=c_weight,\n",
        "                style_weight=s_weight\n",
        "            )\n",
        "            results[(c_weight, s_weight)] = result\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c54ae6",
      "metadata": {
        "id": "63c54ae6"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b967fdfa",
      "metadata": {
        "id": "b967fdfa"
      },
      "source": [
        "4.1 to visulaise the orignal imgaes(opintail)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "56bc9d62",
      "metadata": {
        "id": "56bc9d62"
      },
      "outputs": [],
      "source": [
        "def display_results(original, style, results):\n",
        "    \"\"\"عرض الصور الأصلية والمحولة جنبًا إلى جنب\"\"\"\n",
        "    fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
        "\n",
        "    ax[0, 0].imshow(to_pil_image(original[0]))\n",
        "    ax[0, 0].set_title(\"Original Content\")\n",
        "    ax[0, 1].imshow(to_pil_image(style[0]))\n",
        "    ax[0, 1].set_title(\"Style Reference\")\n",
        "    ax[0, 2].axis('off')\n",
        "\n",
        "    row, col = 1, 0\n",
        "    for name, img in results.items():\n",
        "        ax[row, col].imshow(to_pil_image(img[0]))\n",
        "        ax[row, col].set_title(f\"Style Weight: {name}\")\n",
        "\n",
        "        col += 1\n",
        "        if col > 2:\n",
        "            col = 0\n",
        "            row += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_images_from_folder(folder_path, max_size=None):\n",
        "    images = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            path = os.path.join(folder_path, filename)\n",
        "            img = load_image(path, max_size)\n",
        "            images[filename] = img\n",
        "    return images"
      ],
      "metadata": {
        "id": "r5Wrhb3eHxFY"
      },
      "id": "r5Wrhb3eHxFY",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "72ca2a58",
      "metadata": {
        "id": "72ca2a58"
      },
      "outputs": [],
      "source": [
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.clone().detach()\n",
        "    image = image.squeeze(0)\n",
        "    image = image.numpy().transpose(1, 2, 0)\n",
        "    image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "    image = np.clip(image, 0, 1)\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "c6eada70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "c6eada70",
        "outputId": "64edc6b0-e777-4f5c-a910-aecab2895886"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# تحميل الصور من المجلدات\\ncontent_images = load_images_from_folder(\"/content/con img\", max_size=512)\\nstyle_images = load_images_from_folder(\"/content/style img\", max_size=512)\\n\\n# اختيار الصور\\ncontent_img = content_images[\"natural.png\"]\\nstyle_img = style_images[\"dog.jpg\"]\\n\\n# تنفيذ النقل الأسلوبي مع تحسين الأوزان\\noutput_image = style_transfer(\\n    content_img, style_img,\\n    num_steps=500,\\n    style_weight=1e5,  # توازن أفضل بين الأسلوب والمحتوى\\n    content_weight=1\\n)\\nimshow(output_image, title=\"Stylized Result - Balanced Weights\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "'''\n",
        "# تحميل الصور من المجلدات\n",
        "content_images = load_images_from_folder(\"/content/con img\", max_size=512)\n",
        "style_images = load_images_from_folder(\"/content/style img\", max_size=512)\n",
        "\n",
        "# اختيار الصور\n",
        "content_img = content_images[\"natural.png\"]\n",
        "style_img = style_images[\"dog.jpg\"]\n",
        "\n",
        "# تنفيذ النقل الأسلوبي مع تحسين الأوزان\n",
        "output_image = style_transfer(\n",
        "    content_img, style_img,\n",
        "    num_steps=500,\n",
        "    style_weight=1e5,  # توازن أفضل بين الأسلوب والمحتوى\n",
        "    content_weight=1\n",
        ")\n",
        "imshow(output_image, title=\"Stylized Result - Balanced Weights\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "xTj5xvdsqE9v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "xTj5xvdsqE9v",
        "outputId": "c848e5ee-e161-4fcf-885e-c313cf0c3268"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# تحميل الصور من المجلدات\\ncontent_images = load_images_from_folder(\"/content/con img\", max_size=512)\\nstyle_images = load_images_from_folder(\"/content/style img\", max_size=512)\\n\\n# اختيار الصور\\ncontent_img = content_images[\"portrate.png\"]\\nstyle_img = style_images[\"Portrait – Picasso.png\"]\\n\\n# تنفيذ النقل الأسلوبي مع تحسين الأوزان\\noutput_image = style_transfer(\\n    content_img, style_img,\\n    num_steps=500,\\n    style_weight=1e5,  # توازن أفضل بين الأسلوب والمحتوى\\n    content_weight=1\\n)\\nimshow(output_image, title=\"Stylized Result - Balanced Weights\")\"\"\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "'''\n",
        "# تحميل الصور من المجلدات\n",
        "content_images = load_images_from_folder(\"/content/con img\", max_size=512)\n",
        "style_images = load_images_from_folder(\"/content/style img\", max_size=512)\n",
        "\n",
        "# اختيار الصور\n",
        "content_img = content_images[\"portrate.png\"]\n",
        "style_img = style_images[\"Portrait – Picasso.png\"]\n",
        "\n",
        "# تنفيذ النقل الأسلوبي مع تحسين الأوزان\n",
        "output_image = style_transfer(\n",
        "    content_img, style_img,\n",
        "    num_steps=500,\n",
        "    style_weight=1e5,  # توازن أفضل بين الأسلوب والمحتوى\n",
        "    content_weight=1\n",
        ")\n",
        "imshow(output_image, title=\"Stylized Result - Balanced Weights\")\"\"\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iC96zqU6vT0p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC96zqU6vT0p",
        "outputId": "604575b1-ffe3-4d1a-9727-8d67dc282518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: beach.png + Water Lilies – Monet.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:02<00:00, 195MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50: Content Loss = 0.4043, Style Loss = 0.0000\n",
            "Step 100: Content Loss = 0.3512, Style Loss = 0.0000\n",
            "Step 150: Content Loss = 0.3266, Style Loss = 0.0000\n",
            "Step 200: Content Loss = 0.3116, Style Loss = 0.0000\n",
            "Step 250: Content Loss = 0.3004, Style Loss = 0.0000\n",
            "Step 300: Content Loss = 0.2930, Style Loss = 0.0000\n",
            "Saved to /content/stylized_outputs/beach_with_Water Lilies – Monet.jpg\n",
            "\n",
            "Processing: city.jpg + Starry Night – Van Gogh.jpg\n",
            "Step 50: Content Loss = 5.1864, Style Loss = 0.0001\n",
            "Step 100: Content Loss = 4.3684, Style Loss = 0.0000\n"
          ]
        }
      ],
      "source": [
        "selected_pairs = [\n",
        "    (\"beach.png\", \"Water Lilies – Monet.jpg\"),\n",
        "    (\"city.jpg\", \"Starry Night – Van Gogh.jpg\"),\n",
        "    (\"natural.png\", \"Mosaic Style.jpg\"),\n",
        "    (\"portrate.png\", \"Portrait – Picasso.jpg\"),\n",
        "    (\"street.png\", \"The Scream – Munch.jpg\"),\n",
        "]\n",
        "\n",
        "import os\n",
        "output_dir = \"/content/stylized_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "content_images = load_images_from_folder(\"/content/con img\", max_size=512)\n",
        "style_images = load_images_from_folder(\"/content/style img\", max_size=512)\n",
        "\n",
        "\n",
        "for content_name, style_name in selected_pairs:\n",
        "    print(f\"Processing: {content_name} + {style_name}\")\n",
        "    # Access content_images and style_images\n",
        "    content_img = content_images[content_name]\n",
        "    style_img = style_images[style_name]\n",
        "    output = style_transfer(\n",
        "        content_img, style_img,\n",
        "        num_steps=300, style_weight=1e5, content_weight=1\n",
        "    )\n",
        "\n",
        "    result = output.clone().detach().squeeze(0)\n",
        "    result = result.cpu().numpy().transpose(1, 2, 0)\n",
        "    result = result * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "    result = np.clip(result, 0, 1)\n",
        "    from PIL import Image\n",
        "    result_img = Image.fromarray((result * 255).astype('uint8'))\n",
        "    save_path = os.path.join(output_dir, f\"{content_name.split('.')[0]}_with_{style_name.split('.')[0]}.jpg\")\n",
        "    result_img.save(save_path)\n",
        "    print(f\"Saved to {save_path}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}